{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70845cc6-f24b-4e1a-82cc-bafb6153fb15",
   "metadata": {},
   "source": [
    "# The problem: turn .json file into a usable .csv file\n",
    "\n",
    "`json_sample.json` = 10 record sample of research output Pure API response\n",
    "\n",
    "This is tricksy, as it involves flattening the nested structure of JSON into a CSV. Near certain that to get what I'm after I'll have to code it, anyway, let's see what the AI landscape reckons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ef1db-c359-46ac-863a-dec6fe9892de",
   "metadata": {},
   "source": [
    "## What's there out of the box? Ask AI\n",
    "\n",
    "tl;dr: see above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0443006-bf33-4304-bbc1-b5d80b5da67c",
   "metadata": {},
   "source": [
    "### GitHub Copilot: `csv` and `json` built-in modules\n",
    "\n",
    "Code supplied by GitHub Copilot upon prompt \"is there a library to turn json into csv?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b75aa-6645-47df-bdfe-4f614bc48558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r') as j:\n",
    "        data = json.load(j)  # load json data\n",
    "\n",
    "    # open a file for writing\n",
    "    with open(csv_file, 'w') as c:\n",
    "        writer = csv.writer(c)\n",
    "\n",
    "        # write the headers\n",
    "        writer.writerow(data[0].keys())\n",
    "\n",
    "        # write the data\n",
    "        for item in data:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "# usage\n",
    "json_to_csv('json_sample.json', 'sample_output_gitHub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59747f70-5173-494c-ab4a-7c00c6a54777",
   "metadata": {},
   "source": [
    "Runs into JSONDecodeError. Files are in utf-8.\n",
    "`open()`: if encoding is not specified the encoding used is platform-dependent: `locale.getencoding()` is called to get the current locale encoding. What is that then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbe2b7-0c8b-40c9-ab5e-76072afaaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "locale.getencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dacc51-a0ec-4242-94aa-c33cbf2e8c05",
   "metadata": {},
   "source": [
    "Aha, that ain't what we want, hence the decoding error. Pretty easily fixed by just telling `open()` which encoding to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674d4d4-c9fe-4ca6-9153-b3b8fd29b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as j:\n",
    "        data = json.load(j)  \n",
    "\n",
    "    with open(csv_file, 'w', encoding='utf-8') as c:\n",
    "        writer = csv.writer(c)\n",
    "\n",
    "        writer.writerow(data[0].keys())\n",
    "\n",
    "        for item in data:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "json_to_csv('json_sample.json', 'sample_output_gitHub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249292b4-37f6-4cfc-84f2-ed94b618cee6",
   "metadata": {},
   "source": [
    "`writer.writerow(data[0].keys())` Expects an array at top level, this file doesn't have that, hence the `KeyError`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a288a49-f42e-428c-b8f2-6f8878888b1c",
   "metadata": {},
   "source": [
    "{\n",
    "  \"count\": 93615,\n",
    "  \"pageInformation\": {\n",
    "    \"offset\": 0,\n",
    "    \"size\": 10\n",
    "  },\n",
    "  \"items\": [...\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f5dcb-ef56-471e-88ed-cfcadb4b1e33",
   "metadata": {},
   "source": [
    "OK, might be better to have a bit of an overview of that `.json` file in the first place. How to get that? Detour to [Problem: Get an overview of a JSON file](#overviewJSON)\n",
    "\n",
    "Anyway, this is jucky for two reasons:\n",
    "1. headers are extracted from the first record, so if the records don't have exactly the same this is going to be messy, and, you guessed it, the data I work with is 99,9% like that\n",
    "2. It ain't working with a root of object type and because of 1. I can't be bothered to even think about how to fix this.\n",
    "\n",
    "Should say that I reran GitHub Copilot with the ammended prompt used below and got pretty much the same answer from it as from Microsoft Copilot.\n",
    "\n",
    "Other suggestions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44cc86-8f30-49c3-a28c-b09b1158e032",
   "metadata": {},
   "source": [
    "### ChatGPT: also `csv` and `json` built-in modules\n",
    "\n",
    "Prompt: Write a script to turn a JSON document into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3eef47-cf2c-46f5-9ef8-6d5d4e2d0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(input_file, output_file):\n",
    "    with open(input_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract headers from the keys of the first item\n",
    "    headers = list(data[0].keys())\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = input(\"Enter the path to the input JSON file: \")\n",
    "    output_file = input(\"Enter the path to the output CSV file: \")\n",
    "    json_to_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451af4a-12e6-4281-85dc-b88c7c034d40",
   "metadata": {},
   "source": [
    "Mh, same header problem again. But some different code for the writing to file. `DictWriter`, what's that? [https://docs.python.org/3.11/library/csv.html#csv.DictWriter](https://docs.python.org/3.11/library/csv.html#csv.DictWriter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39d837-d8d8-4530-b159-2d5c4d5d8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', dialect='excel', *args, **kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45096f0-c660-4fa3-9ba3-ed5c56fcbc1e",
   "metadata": {},
   "source": [
    "Create an object which operates like a regular writer but maps dictionaries onto output rows. The `fieldnames` parameter is a `sequence` of keys that identify the order in which values in the dictionary passed to the `writerow()` method are written to file `f`. The optional `restval` parameter specifies the value to be written if the dictionary is missing a key in `fieldnames`. If the dictionary passed to the `writerow()` method contains a key not found in `fieldnames`, the optional `extrasaction` parameter indicates what action to take. If it is set to `'raise'`, the default value, a `ValueError` is raised. If it is set to `'ignore'`, extra values in the dictionary are ignored. Any other optional or keyword arguments are passed to the underlying `writer` instance.\n",
    "\n",
    "That sounds rather promising, let's keep this in mind!\n",
    "\n",
    "Overall, though we have the same header extraction problem as before, so not even going to try that one out.\n",
    "\n",
    "Anyone else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5ce9f-b79b-4d88-8687-7a1dcd7d1a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Microsoft Copilot: `pandas` or `csv` and `json`\n",
    "\n",
    "Ah, that `pandas` approach is new, let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc88ee-b05d-40d9-94db-2bf073e67a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON data from a file (replace 'jsonfile.json' with your actual JSON file)\n",
    "with open('json_sample.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "\n",
    "# Write the data to a CSV file (replace 'csvfile.csv' with your desired output file name)\n",
    "df.to_csv('sample_output_MSCopilot.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223df9c-0721-4d0b-a450-78f3f56a5ed3",
   "metadata": {},
   "source": [
    "Mh, nothing happening? Running it in PyCharm ran into this error: `ValueError: Mixing dicts with non-Series may lead to ambiguous ordering.` Whatever that is, I'm not convinced this is the way forward anyway, so qhat's the `csv` and `json` solution suggested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8063d2-226e-4043-9ba7-fe26443ef665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the JSON data from a file (replace 'data.json' with your actual JSON file)\n",
    "with open('data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extract relevant fields (assuming your JSON structure is a list of dictionaries)\n",
    "fieldnames = ['pk', 'model', 'codename', 'content_type', 'name']\n",
    "\n",
    "# Write the data to a CSV file (replace 'data.csv' with your desired output file name)\n",
    "with open('data.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in data:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a49fa-18f5-4cb4-86d3-4d332ca8bff0",
   "metadata": {},
   "source": [
    "Pretty much what we've seen before. The second comment is interesting though: `Extract relevant fields (assuming your JSON structure is a list of dictionaries)`. That's exactly the type of assumption we can't make. \n",
    "\n",
    "Let's change tact: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d40af-c5ef-4f37-9e93-9f577f0a0620",
   "metadata": {},
   "source": [
    "### What if I don't have a list of dictionaries data structure in the JSON file?\n",
    "\n",
    "GitHub Copilot tells me that: \"if your JSON file contains a dictionary of dictionaries, you can still convert it to a CSV file. However, the keys of the outer dictionary will be lost in the conversion\". Jup, that's why it's tricky in the first place.\n",
    "\n",
    "ChatGPT says: \"If your JSON file contains \\[...] nested objects or arrays of arrays, you may need to handle it differently. One common approach is to flatten the nested structure before converting it to a CSV format.\"\n",
    "\n",
    "Microsoft Copilot reckons: \"If your JSON contains nested objects (not a flat list of dictionaries), you can flatten it before converting to CSV.\"\n",
    "\n",
    "OK, well, I knew that the nesting would be hard from the start. Tired of rooting around. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9f0c9-a04d-463c-b75c-b43ef72c8a96",
   "metadata": {},
   "source": [
    "### How it would work with regular data\n",
    "\n",
    "If the data was regular the above script would have of course worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687639e7-dc38-4128-afcf-6044200b53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as j:\n",
    "        data = json.load(j)  \n",
    "\n",
    "    with open(csv_file, 'w', encoding='utf-8') as c:\n",
    "        writer = csv.writer(c)\n",
    "\n",
    "        writer.writerow(data[0].keys())\n",
    "\n",
    "        for item in data:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "json_to_csv('json_regular.json', 'regular_sample_output_gitHub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cee93f-ad5b-44fb-a769-5f233c0aca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract headers from the keys of the first item\n",
    "    headers = list(data[0].keys())\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "json_to_csv('json_regular.json', 'regular_sample_output_chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594e7e11-5bde-484a-8f26-7f899e6d3ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>age</th>\n",
       "      <th>home_town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>25</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "      <td>22</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jean</td>\n",
       "      <td>Dupont</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max</td>\n",
       "      <td>Mustermann</td>\n",
       "      <td>28</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Otto</td>\n",
       "      <td>Normalverbraucher</td>\n",
       "      <td>35</td>\n",
       "      <td>Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jón</td>\n",
       "      <td>Jónsson</td>\n",
       "      <td>40</td>\n",
       "      <td>Rejkjavik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Numerius</td>\n",
       "      <td>Negidius</td>\n",
       "      <td>12</td>\n",
       "      <td>Rome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anna</td>\n",
       "      <td>Kowalska</td>\n",
       "      <td>50</td>\n",
       "      <td>Warsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vardenis</td>\n",
       "      <td>Pavardenis</td>\n",
       "      <td>26</td>\n",
       "      <td>Vilnius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jens</td>\n",
       "      <td>Jensen</td>\n",
       "      <td>34</td>\n",
       "      <td>Copenhagen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  firstname           lastname  age   home_town\n",
       "0      John                Doe   25      London\n",
       "1      Jane                Doe   22    New York\n",
       "2      Jean             Dupont   30       Paris\n",
       "3       Max         Mustermann   28      Berlin\n",
       "4      Otto  Normalverbraucher   35     Hamburg\n",
       "5       Jón            Jónsson   40   Rejkjavik\n",
       "6  Numerius           Negidius   12        Rome\n",
       "7      Anna           Kowalska   50      Warsaw\n",
       "8  Vardenis         Pavardenis   26     Vilnius\n",
       "9      Jens             Jensen   34  Copenhagen"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON data from a file (replace 'jsonfile.json' with your actual JSON file)\n",
    "with open('json_regular.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "\n",
    "# Write the data to a CSV file (replace 'csvfile.csv' with your desired output file name)\n",
    "df.to_csv('regular_sample_output_ms.csv', encoding='utf-8', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4e3d6f-614c-4a25-a2f7-9c4c4bf27d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>home_town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'firstname': 'John', 'lastname': 'Doe'}</td>\n",
       "      <td>25</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'firstname': 'Jane', 'lastname': 'Doe'}</td>\n",
       "      <td>22</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'firstname': 'Jean', 'lastname': 'Dupont'}</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'firstname': 'Max', 'lastname': 'Mustermann'}</td>\n",
       "      <td>28</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'firstname': 'Otto', 'lastname': 'Normalverbr...</td>\n",
       "      <td>35</td>\n",
       "      <td>Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'firstname': 'Jón', 'lastname': 'Jónsson'}</td>\n",
       "      <td>40</td>\n",
       "      <td>Rejkjavik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'firstname': 'Numerius', 'lastname': 'Negidius'}</td>\n",
       "      <td>12</td>\n",
       "      <td>Rome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'firstname': 'Anna', 'lastname': 'Kowalska'}</td>\n",
       "      <td>50</td>\n",
       "      <td>Warsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'firstname': 'Vardenis', 'lastname': 'Pavarde...</td>\n",
       "      <td>26</td>\n",
       "      <td>Vilnius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'firstname': 'Jens', 'lastname': 'Jensen'}</td>\n",
       "      <td>34</td>\n",
       "      <td>Copenhagen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  age   home_town\n",
       "0           {'firstname': 'John', 'lastname': 'Doe'}   25      London\n",
       "1           {'firstname': 'Jane', 'lastname': 'Doe'}   22    New York\n",
       "2        {'firstname': 'Jean', 'lastname': 'Dupont'}   30       Paris\n",
       "3     {'firstname': 'Max', 'lastname': 'Mustermann'}   28      Berlin\n",
       "4  {'firstname': 'Otto', 'lastname': 'Normalverbr...   35     Hamburg\n",
       "5        {'firstname': 'Jón', 'lastname': 'Jónsson'}   40   Rejkjavik\n",
       "6  {'firstname': 'Numerius', 'lastname': 'Negidius'}   12        Rome\n",
       "7      {'firstname': 'Anna', 'lastname': 'Kowalska'}   50      Warsaw\n",
       "8  {'firstname': 'Vardenis', 'lastname': 'Pavarde...   26     Vilnius\n",
       "9        {'firstname': 'Jens', 'lastname': 'Jensen'}   34  Copenhagen"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON data from a file (replace 'jsonfile.json' with your actual JSON file)\n",
    "with open('json_rnest.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "\n",
    "# Write the data to a CSV file (replace 'csvfile.csv' with your desired output file name)\n",
    "df.to_csv('rnest_sample_output_ms.csv', encoding='utf-8', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a013055-99dd-47ae-a328-c77307b4f56b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "1. Which headers are expected? -> [Problem: Get an overview of a JSON file](#overviewJSON) might help\n",
    "2. How to handle nesting? Is there a level that should be the \"base-level\" against which to flatten? How to express nested values in the CSV?\n",
    "3. Is there a list in the .json file that can be used to make the above work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075cfe8-7a8d-4ca3-bd1e-c04c371a43d6",
   "metadata": {},
   "source": [
    "## Potential solution\n",
    "\n",
    "1. Use the `analyse_json.py` script to get paths and extract expected headers from them\n",
    "2. Define a baseline and target list agains which to flatten. Decided to flatten the sample so each author gets their own line, which means in cases where there is more than one author item information needs to be repeated. So the `root.items` list is the baseline, and `root.items.[].contributors` the target.\n",
    "\n",
    "Re point 1, removing the `root` element from the output is not great for reusing the output, removed it for this purpose. Also this time I want the paths sorted so things don't end up all over the place in the output, so added a sort in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8824be8d-b4d1-4d0b-b1ee-4cade5cd89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import analyse_json2 \n",
    "import json_to_csv\n",
    "\n",
    "analyse_json2.run('json_sample.json', 'sample_analysis2.csv', sort=True)\n",
    "json_to_csv.run('json_sample.json', 'sample_analysis2.csv', 'sample_to_csv.csv', 'root.items', 'root.items.[].contributors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f483eb-6c95-45a2-8544-d77e7ec4f0bc",
   "metadata": {},
   "source": [
    "Input for the `run()` function are the path to the .json file (`'json_sample.json'`), the analysis file (`'sample_analysis2.txt'`), the base line list name (`'root.items'`) and the target list name (`'root.items.[].contributors'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d8bbf-32da-4a67-828e-edd95a7a2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(json_file, header_file, base, target, encoding='utf-8'):\n",
    "    headers, lists = _get_headers_depth(header_file)\n",
    "    with open(json_file, 'r', encoding=encoding) as file:\n",
    "        try:\n",
    "            data = json.load(file)\n",
    "            result = _traverse(data, 'root', base, target, lists, None)\n",
    "        except JSONDecodeError:\n",
    "            print('JSONDecodeError')\n",
    "    _format_output(result, base, target, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584a561-3bd1-4149-b748-3530d3ce4a35",
   "metadata": {},
   "source": [
    "### `_get_headers()` and `_check_intersection()`\n",
    "\n",
    "First, get all the elements that will be headers in the output, i.e. the paths that lead to actual values rather than an array or object. Also, I want a dictionary of all the list paths as keys and as their values a dictionary with all their subpaths as keys with blank values. This will later be used to ensure each path is visited for each child of those lists. \n",
    "\n",
    "Extract the datatype and path from the json analysis and if it encounters a \"list\" add the path to the, at this stage, list of lists. If the datatype of the path is not in the `exclusions` the path gets added to `headers` else it's just passed over.\n",
    "\n",
    "Finally the list of lists is turned into dictionary of dictionaries that'll be needed for list traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df04cc2-f89d-42fc-ad55-dedc5b57cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_headers(file_path, has_headers=True, path_index=0, dt_index=2, exclusions=('dict', 'list'),\n",
    "                 encoding='utf-8'):\n",
    "    headers = []\n",
    "    lists_list = []\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        reader = csv.reader(file)\n",
    "        for index, row in enumerate(reader):\n",
    "            if index == 0:\n",
    "                if has_headers:\n",
    "                    continue\n",
    "            datatypes = row[dt_index].split(';')\n",
    "            path = row[path_index]\n",
    "            if 'list' in datatypes:\n",
    "                lists_list.append(path)\n",
    "            if _check_intersection(datatypes,\n",
    "                                   exclusions):  # todo: this is not fool proof if say an element can be list or string\n",
    "                pass\n",
    "            else:\n",
    "                headers.append(path)\n",
    "    lists = {}\n",
    "    for li in lists_list:\n",
    "        lists[li] = {}\n",
    "        for h in headers:\n",
    "            if li in h:\n",
    "                lists[li][h] = ''\n",
    "    return headers, lists\n",
    "\n",
    "\n",
    "def _check_intersection(list1, list2):\n",
    "    return len(set(list1) & set(list2)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1356539-23d2-4f62-8a45-765e324b9de1",
   "metadata": {},
   "source": [
    "### `_traverse()` and `_format_value()`\n",
    "\n",
    "This is the same basic traversal as with the analysis. The `out` parameter gets passed around, it'll contain the result, at the start there is None. Can't have an empty `dict` as default parameter, hence the if `None` then `{}` assignment.\n",
    "\n",
    "If traversing a list that's either the baseline or the target it needs different treatment, hence the `is_target` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0ceb6-b003-40ff-8247-d1e0f9978a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _traverse(root, name: str, base: str, target_name: str, lists_list, out=None):\n",
    "        if out is None:\n",
    "        out = {}\n",
    "    is_target = False\n",
    "    if name == target_name or name == base:\n",
    "        is_target = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89984d-679f-43d7-8ea2-ca9085805b2e",
   "metadata": {},
   "source": [
    "If traversing a `dict`/object run through the children. Names for children are constructed the same way as in the analysis script. This time though I need to reuse `name` later on without the child extension, hence split into `name` and `name_child`.\n",
    "\n",
    "If the traversal of a child comes back with a literal, add that to `out` with `name_child` as key.\n",
    "\n",
    "If the child traversed is a list, unpack the `ret` dict into the `out` dict. \n",
    "\n",
    "Finally increment the counter variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ee527-4697-49b3-ae70-249fa89583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if isinstance(root, dict):\n",
    "        i = 0  # counter variable for sibling elements\n",
    "        for child in root:  # cycle through the children of root element\n",
    "            name_child = name + \".\" + child\n",
    "            ret = _traverse(root[child], name_child, base, target_name, lists_dict, out=out)\n",
    "            if isinstance(ret, str):\n",
    "                out[name_child] = ret\n",
    "            if isinstance(root[child], list):\n",
    "                out = _unpack(out, ret, is_target)\n",
    "            i = i + 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53480af-8f18-4a4e-aa90-7331a729f5d7",
   "metadata": {},
   "source": [
    "If traversing a `list`/array grab a deep copy of the dictionary with this lists children as keys. Reason is that not all paths may be present in all children of the list, but I need a return value for each path, so I can later access them via index. If the path isn't present in the child then the return value is an empty string.\n",
    "\n",
    "Now run through the children. This time we don't want to pass the same `out` variable as before as the result will be different for each child and they'd overwrite each other, so each child get's its own `_out`. \n",
    "\n",
    "If the child traversal brings back a literal it means we're dealing with a list like `[\"a\", \"b\", \"c\"]` and it just get's dumped back into an semicolon separated list of literal values under the respective path name. Semicolon as I don't want them to get mixed up with the comma separation.\n",
    "\n",
    "Otherwise, we need to incorporate the traversal result into the `list_headers` dict.\n",
    "\n",
    "Finally increment the counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d648384-7624-4718-b9b1-a05e010d6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "    elif isinstance(root, list):  \n",
    "        i = 0  # counter variable for sibling elements\n",
    "        list_headers = deepcopy(lists_dict[name])\n",
    "        for child in root:  # cycle through the children of root element\n",
    "            _out = {}\n",
    "            name_child = name + '.[]'\n",
    "            ret = _traverse(child, name_child, base, target_name, lists_dict, out=_out)\n",
    "            if isinstance(ret, str):\n",
    "                try:\n",
    "                    prev_value = out[name]\n",
    "                    if prev_value != '':\n",
    "                        out[name] = prev_value + ';' + ret\n",
    "                except KeyError:\n",
    "                    out[name] = ret\n",
    "            else:\n",
    "                list_headers = _join(list_headers, _out, i, is_target)\n",
    "\n",
    "            i = i + 1\n",
    "        return list_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb519745-5f95-4f4d-b912-f3a310d52bfd",
   "metadata": {},
   "source": [
    "If the traversal encounters a literal value (anything but `list` or `dict` basically), format that value into a string and, if it contains a comma double-quote it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9178d91-94c9-4f51-a7f5-6854af07a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    else:\n",
    "        return _format_value(root)\n",
    "\n",
    "def _format_value(data_to_write):\n",
    "    if isinstance(data_to_write, str):\n",
    "        if ',' in data_to_write:\n",
    "            data_to_write = '\"' + data_to_write + '\"'\n",
    "        return data_to_write\n",
    "    if isinstance(data_to_write, int) or isinstance(data_to_write, float) or isinstance(data_to_write, bool):\n",
    "        return str(data_to_write)\n",
    "    if data_to_write is None:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bb614-152c-44fa-8d68-59d113c5e240",
   "metadata": {},
   "source": [
    "### `_unpack()`\n",
    "\n",
    "This is merges a child `dict` into the parent `dict`, where the keys do not already exist in `parent_dict` as this was a `dict` iteration. Iterate over the keys in `child_dict`. \n",
    "\n",
    "`parent_dict` should **never** already contain any of the keys in `child_dict` hence the Error if not.\n",
    "\n",
    "If the value of the respective key is a `list` the length of the list tells how it should be treated. If it has a length of `1`, just grab that value and dump it as the value for the key. If it's longer than `1`, i.e. this is either the base list or target list, dump the list as it is as the value. Empty lists should not happen hence the error if it does.\n",
    "\n",
    "`child_dict = {\"key\": [\"bli;bla;blubb\"]}` -> literals that got concatenate, i.e. that will not be expanded into their own rows\n",
    "\n",
    "`child_dict = {\"key\": [bli, bla, blubb]}` -> literals that will get expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103efe68-f0f3-4ee8-ab7f-891856e695e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(parent_dict, child_dict):\n",
    "    for key in child_dict.keys():\n",
    "        if key not in parent_dict.keys():\n",
    "            if isinstance(cv := child_dict[key], list):\n",
    "                if len(cv) > 1:\n",
    "                    parent_dict[key] = cv\n",
    "                elif len(cv) == 1:\n",
    "                    parent_dict[key] = cv[0]\n",
    "                else:\n",
    "                    raise ValueError('Empty list')\n",
    "            else:\n",
    "                parent_dict[key] = child_dict[key]\n",
    "        else:\n",
    "            raise ValueError('Key already exists')\n",
    "    return parent_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafa913-613e-484d-ba22-4205841b5a7d",
   "metadata": {},
   "source": [
    "### `_join()`\n",
    "\n",
    "This one is merging list child values into the list parent values. Keys will already exist in `parent_dict` as it from the start contained all possible keys in the list.\n",
    "\n",
    "Iterate over the `parent_dict` keys and try the same key on the `child_dict`. If the path didn't exist in the respective child this will throw a `KeyError` in which case the child's value for this key is an empty string. Else the child's value is whatever the `child_dict` has.\n",
    "\n",
    "If this is the first child (`index == 0`), no need for any fuss, just dump the child value into the respective key in the `parent_dict`. If this is a subsequent child however a decision needs to be made as to how to append the value. \n",
    "\n",
    "First, let's grab the `parent_value`, as this will change before it get's written back to the `parent_dict` and that can't be done with it in situ.\n",
    "\n",
    "Are we dealing with either the base or the target level? If so, the `parent_value` should be a `list` already, if not there's something off. Append the `child_value` and write the new `parent_value` back to the `parent_dict`.\n",
    "\n",
    "If we're not dealing with base or target level then no appending, but concatenating. Child values at this stage are `lists`, so we need to grab the string from it. There should be only one entry in the list anyway. Concatenate the `child_value`, `;` and `parent_value` and write the lot back to the `parent_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da12bc9-00ab-47bc-a1d7-7ea1b9ce1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join(parent_dict, child_dict, index, is_target_level=False):\n",
    "    for key in parent_dict.keys():\n",
    "        try:\n",
    "            child_value = child_dict[key]\n",
    "        except KeyError:\n",
    "            child_value = ''\n",
    "        if index == 0:\n",
    "            parent_dict[key] = [child_value]\n",
    "        else:\n",
    "            parent_value = parent_dict[key]\n",
    "            if is_target_level:\n",
    "                if not isinstance(parent_value, list):\n",
    "                    raise ValueError('Not a list')\n",
    "                else:\n",
    "                    parent_value.append(child_value)\n",
    "                    parent_dict[key] = parent_value\n",
    "            else:\n",
    "                if not isinstance(parent_value, str):\n",
    "                    parent_value = parent_value[0]\n",
    "                else:\n",
    "                    raise ValueError('Not a string')\n",
    "                new_value = parent_value + ';' + child_value\n",
    "                parent_dict[key] = [new_value]\n",
    "    return parent_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7517cc-c4fd-4398-81e6-9d3965d3b3e4",
   "metadata": {},
   "source": [
    "### `_format_output()` and `_output()`\n",
    "\n",
    "First write the headers to file.\n",
    "\n",
    "Then we need to separate the headers into those that will always contain the same values (`constants`) as they are above the base level, those that are at base level (`base_fields`) and those that are at target level (`target_fields`). We'll do this by evaluating the path names in the `headers` list. The base and target list paths won't appear in the `headers` though as they are paths for `lists` not for literals, so let's append the list marker to them so they can be matched to values in `headers`.\n",
    "\n",
    "Now iterate over the values in `headers`. The order of the if statements matters here. Fishing out the constants is easy as they are the only ones that at this stage will have literal values. Then the `target_fields` as they will also match for having `base_list` in their path and we need to get them out of the way before that can happen. Finally all else should end up in the `base_fields` list, if not give me an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d289cb-fb62-48c4-a98d-3bee7cc75dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_output(result, base, target, headers):\n",
    "    output(None, headers, head=True)\n",
    "    constants = []\n",
    "    base_fields = []\n",
    "    target_fields = []\n",
    "    base_list = base + '.[]'\n",
    "    target_list = target + '.[]'\n",
    "    for h in headers:\n",
    "        value = result[h]\n",
    "        if not isinstance(value, list):\n",
    "            constants.append(h)\n",
    "            continue\n",
    "        if target_list in h:\n",
    "            target_fields.append(h)\n",
    "            continue\n",
    "        if base_list in h:\n",
    "            base_fields.append(h)\n",
    "            continue\n",
    "        raise ValueError('Field not a constant, in base or target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54782846-6393-4155-891e-6788379515f9",
   "metadata": {},
   "source": [
    "Now, we need to know how long our base list is, easy, we just grab a value from `base_fields`, retrieve the value associated with that key from results and see how many values are in that array. That length is the same for all keys in `base_fields`, the `join()` function made sure of that by assigning a value for each path for each child. \n",
    "\n",
    "Now set up counter and the `dict` that will contain the data for an individual row.\n",
    "\n",
    "First populate this with the constant values. That never needs repeating as they never change again, the value is the same for each row. \n",
    "\n",
    "Then we iterate through keys in `base_fields` and add the value from the respective array slot to `row`.\n",
    "\n",
    "Finally for the `target_fields`. First we check if there is an array of values to deal with, as will be the case if there is one than more author). If so we need the length of that array (`target_length`) and a counter (`target_counter`). Then we iterate over the `target_fields` and retrieve the respective value as the array slot annd add it to `row`. \n",
    "\n",
    "Now the row is ready for `_output()`. Increment the counter and process the next array slot. This means the `target_fields` values change but everything else stays the same. \n",
    "\n",
    "If there isn't a list to deal with, then just iterate over `target_fields`, extract the values and output the `row`.\n",
    "\n",
    "Then on to the next `base_field` and repeat the spiel until all have been processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c55b2-b9c3-4389-b878-bf372ad57ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    base_length = len(result[base_fields[0]])\n",
    "    base_counter = 0\n",
    "    row = {}\n",
    "    for constant in constants:\n",
    "        row[constant] = result[constant]\n",
    "    while base_counter < base_length:\n",
    "        for base_field in base_fields:\n",
    "            row[base_field] = result[base_field][base_counter]\n",
    "        if isinstance(x := result[target_fields[0]][base_counter], list):\n",
    "            target_length = len(x)\n",
    "            target_counter = 0\n",
    "            while target_counter < target_length:\n",
    "                for target_field in target_fields:\n",
    "                    row[target_field] = result[target_field][base_counter][target_counter]\n",
    "                output(row, headers)\n",
    "                target_counter += 1\n",
    "        else:\n",
    "            for target_field in target_fields:\n",
    "                row[target_field] = result[target_field][base_counter]\n",
    "            output(row, headers)\n",
    "        base_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516026f3-b5e3-4fe3-8a28-db09818cb377",
   "metadata": {},
   "source": [
    "This one just handles the writing to file of rows, using the `csv.DictWriter`. One can feed this a list of headers and then dictionaries of those headers as keys and it then concatenates the values into the right order of comma delimited values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006428aa-ee9d-414a-9e43-18ba8a9de907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _output(_data, headers, head=False):\n",
    "    with open('resources/sample_out.txt', 'a', encoding='utf-8', newline='') as out_file:\n",
    "        writer = csv.DictWriter(out_file, fieldnames=headers)\n",
    "        if head:\n",
    "            writer.writeheader()\n",
    "        else:\n",
    "            writer.writerow(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baee38-9507-4ef6-9bfc-2306ec249ac3",
   "metadata": {},
   "source": [
    "<a id=\"overviewJSON\"></a>\n",
    "## Problem: Get an overview of a JSON file\n",
    "\n",
    "I want to know:\n",
    "- what **paths** are in the file,\n",
    "- what their **value data type** is and\n",
    "- **how often** they occur.\n",
    "\n",
    "The tools I found and tried all didn't allow me to get the data I wanted. \n",
    "\n",
    "- [jq](https://jqlang.github.io/jq/), a command-line JSON processor, does have a function(s) to output paths, but they aren't quite what I'm after. Maybe I could have fiddled with this and made it work, but then I may as well just write it in Python in the first place.\n",
    "- Notepad++ extensions I tried also didn't do the right thing, they have tree views to get a better overview of the file, but don't do any of the stats things I want\n",
    "\n",
    "By this point I'd spent quite some time trying to find an existing solution and ran out of things to google, so decided to write it myself. See `analyse_json.py`.\n",
    "\n",
    "Here's what it does with the the `json_sample.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703572ce-f624-4df4-bf72-35c69dbd0f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import analyse_json\n",
    "\n",
    "analyse_json.run('json_sample.json', 'sample_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c00cd-d69a-4c71-99d8-268b5e9d7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyse_json_gui\n",
    "\n",
    "analyse_json_gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8615582-5838-4489-b8b2-c924762814ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import analyse_json\n",
    "\n",
    "analyse_json.run('json_sample.json', 'sample_analysis.csv')\n",
    "\n",
    "import pandas\n",
    "\n",
    "analysis = pandas.read_csv('sample_analysis.csv')\n",
    "pandas.set_option('display.max_rows', 200)\n",
    "pandas.set_option('display.colheader_justify','left')\n",
    "analysis = analysis.style.set_properties(**{'text-align': 'left', 'font-family': 'Monospace'})\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c00ce-77a8-422e-a710-fc01d8850ed2",
   "metadata": {},
   "source": [
    "### `run()`\n",
    "\n",
    "The `run()` function (see above) is the only public function and called to start the script. Provide the path to the input and output file as arguments. If needed the encoding scheme needed can be set as well; the default encoding scheme is UTF8.\n",
    "\n",
    "It sets instantiates the `tags` variable which will hold the encountered paths, they counts and data types.\n",
    "It checks that `input_file` and `output_file` file paths are actually provided and calls `set_file()` if one is missing. For `input_file` with the `mandatory` parameter set to `True`. For `input_file` it also checks that the file actually exists and if not calls `_set_file()` to rectify this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d9a5e0-efb9-406b-b9bd-2dc10479f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_file: str = None, output_file: str = None, encoding='utf-8'):\n",
    "    tags = {}\n",
    "    if input_file is None:\n",
    "        input_file = _set_file('file to analyse', True)\n",
    "    if not os.path.isfile(input_file):\n",
    "        print('Provided input file does not exist.')\n",
    "        input_file = _set_file('file to analyse', True)\n",
    "    if output_file is None:\n",
    "        output_file = _set_file('output file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22706f0a-ad9b-4be4-8f6b-d05b74c7c6bb",
   "metadata": {},
   "source": [
    "If both `input_file` and `output_file` are not `None` it tries to open the `input_file` and load it as json. Should this run into a `JSONDecodeError` it is caught and the script execution stopped. At this point there should be no possibility of `input_file` or `output_file` still being `None`, but just in case there is a hole in my logic...\n",
    "\n",
    "`JSONDecodeError` can happen if the file is not valid JSON or if there are characters in the file that can't be decoded with the provided encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aaccd4-f5e5-43df-8bfa-e8c98c8e7fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    if input_file is not None and output_file is not None:\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding) as file:\n",
    "                data = json.load(file)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\n",
    "                \"Input file could not be decoded. Could be because it's not a valid JSON file or it contains characters that cannot be decoded with the provided encoding scheme. Exiting script. JSONDecodeError: \" + str(\n",
    "                    e))\n",
    "            raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144f1a3-ae3e-4ac4-a695-f68e29d42119",
   "metadata": {},
   "source": [
    "Next the `traverse()` function is called to run through the actual analysis. `'root'` is provided as the value for the `name` paramenter as it needs a name of some sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753afb5-1237-4897-91cf-033fc18c5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "        _traverse(data, 'root', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cebde2-8536-4b7a-b3df-0dc5e50e31fc",
   "metadata": {},
   "source": [
    "Finally, the `output()` function is called to output the data now held in the `tags` variable. Enclosed it in a `try/except` block to catch any issue with writing to the provided path and rather than running into trouble the script dumps the value of `tags` into a cache file at root level and gracefully exits. \n",
    "\n",
    "Right at the end is the graceful exit if `input_file` or `output_file` were still `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d87c-aa45-43a0-8f80-d357440c0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "        try:\n",
    "            _output(tags, output_file, encoding)\n",
    "        except IOError as e:\n",
    "            print(\"Output file could not be written. Dumping to cache and exiting script. IOError: \" + str(e))\n",
    "            with open('cache.txt', 'w', encoding=encoding) as cache:\n",
    "                cache.write(str(tags))\n",
    "            raise StopExecution\n",
    "    else:\n",
    "    print(\"Input file or output file missing. Exiting script.\")\n",
    "    raise StopExecution   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88eac4-b945-441c-a534-1deb72bc00db",
   "metadata": {},
   "source": [
    "### `traverse()`\n",
    "\n",
    "The main function is `traverse()`. It recursively traverses the entire document and fishes out all paths it encounters, triggers recording their value data type and counting. Its paramenter are the element is is evaluating (`root`), its name/path (`name`) and the `tags` dictionary containing all previously encountered paths, they counts and data types. \n",
    "\n",
    "It only needs to act recursively is the passed in element is either a `list` or `dict`. In this case it sets up a counter for sibling elements and then loops over the children in the respective `list` or `dict`. \n",
    "\n",
    "For `root`s that are `dict` the name of the child element preceded by `.` is appended to `name` and the `traverse()` function called with the child as the `root` parameter value.\n",
    "\n",
    "For `root`s that are `list` `.[]` is appended to the name as there is no key to use and the `traverse()` function called with the child as the `root` parameter value.\n",
    "\n",
    "Finally, the sibling counter is incremented.\n",
    "\n",
    "If the current child is a sibling, the script first removes the previously appended `name` part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7becd-fc80-4180-a5f2-4e3dbac41cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _traverse(root, name: str, tags: dict) -> None:\n",
    "    _record_datatype(name, tags, type(root))\n",
    "    _count_tag(name, tags)\n",
    "    if isinstance(root, list) | isinstance(root, dict):  # check if passed element is a list or dictionary\n",
    "        i = 0                                            # counter variable for sibling elements\n",
    "        for child in root:                               # cycle through the children of root element\n",
    "            if i > 0:                                    # i > 0 means this is sibling element and last name element needs removing\n",
    "                try:\n",
    "                    name = name[:name.rindex(\".\")]\n",
    "                except ValueError:\n",
    "                    name = name\n",
    "                    print('ValueError: ' + name)\n",
    "\n",
    "            if isinstance(root, list):\n",
    "                name = name + '.[]'\n",
    "                _traverse(child, name, tags)\n",
    "\n",
    "            else:\n",
    "                name = name + \".\" + child\n",
    "                _traverse(root[child], name, tags)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9dace-b1bc-436c-85b5-284daaedad66",
   "metadata": {},
   "source": [
    "### `_count_tag()`, `_record_datatype()` and `setup()`\n",
    "\n",
    "`_count_tag()` takes an element name/path and the `tags` dictionary as paramenters. It tries to grab a previous count value for the respective path and increments it. If there is no previous count because this path hasn't been previously encountered it calls `_setup` to instantiate the respective dictionary holding count and data types within itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf03b1-d770-448d-9c86-daa1b10a17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_tag(tag_name: str, tags: dict) -> None:\n",
    "    try:\n",
    "        count = tags[tag_name]['count']\n",
    "        tags[tag_name]['count'] = count + 1\n",
    "    except KeyError:\n",
    "        tags[tag_name] = _setup(count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7469296-355e-4726-9b1f-08e25cc4462e",
   "metadata": {},
   "source": [
    "`_record_datatype()` also takes an element name/path and the `tags` dictionary as paramenters, plus the element's [`Type`](https://docs.python.org/3.11/library/stdtypes.html#type-objects). First it turns the `Type` objects string representation into something nicer to read. It then also tries to grab any previously deposited data type values and, if the respective one isn't already in there, append it to them. If there are no previous values, because the path hasn't been previously encountered it calls `_setup` to instantiate the respective dictionary holding count and data types within itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa89a68-92a4-443f-ab04-8744d348e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _record_datatype(tag_name, tags: dict, datatype) -> None:\n",
    "    datatype = str(datatype).replace(\"<class '\", \"\").replace(\"'>\", \"\")\n",
    "    try:\n",
    "        datatypes = tags[tag_name]['datatypes']\n",
    "        if datatype not in datatypes:\n",
    "            datatypes.append(datatype)\n",
    "    except KeyError:\n",
    "        tags[tag_name] = _setup(datatype=datatype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfe8c9-5612-40ca-915f-ed7580c32983",
   "metadata": {},
   "source": [
    "`_setup()` instantiates the dictionary holding count and data types for a path within the `tags` dictionary. It can be called either with an initial count value or an initial data type value. An initial `count` value is always 0, but `datatypes` and either be an empty `list` (if the call is coming from `_count_tag()` or have an initial value (if the call is coming from `_record_dataype()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cc6a6-a58d-4f10-b749-7c8520bb3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setup(count=0, datatype=None):\n",
    "    d = {'count': count}\n",
    "    if datatype is not None:\n",
    "        d['datatypes'] = [datatype]\n",
    "    else:\n",
    "        d['datatypes'] = []\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8032d-eb1c-43e7-a48a-30558d3cf356",
   "metadata": {},
   "source": [
    "### `_output()` and `_format_list_content`\n",
    "\n",
    "`_output()` handles writing the `tags` dictionary to a file in CSV. Once the file is opened it writes the header line and then iterates through `tags`. \n",
    "\n",
    "For each entry in `tags` it retrieves the path, i.e. the key to the inner dictionary holding the count and datatypes. It only keeps `'root.'` for the actual root element, else everything would start in 'root' and I just find that ugly. \n",
    "\n",
    "Then it grabs the `list` holding the data types gets it formatted.\n",
    "\n",
    "Finally it concatenates path, count, data types into a comma-delimited output string and writes it to file.\n",
    "\n",
    "Just added the \"Done\" print as otherwise there is no indication in the notebook of the script having finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b2a41-092e-4e88-8f30-ed059e2ca678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _output(tag_set: dict, out_file_path, encoding) -> None:\n",
    "    with open(out_file_path, 'w', encoding=encoding) as out_file:\n",
    "        out_file.write(\"tag,count,datatypes\\n\")\n",
    "        for tag in tag_set:\n",
    "            tag_out = 'root'\n",
    "            if tag != tag_out:\n",
    "                tag_out = tag.replace('root.', '')\n",
    "            datatypes = _format_list_content(tag_set[tag]['datatypes'])\n",
    "            out_file.write(tag_out + \",\" + str(tag_set[tag]['count']) + ',' + datatypes + \"\\n\")\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f7b14-46da-418d-babb-f526b4d1e9fe",
   "metadata": {},
   "source": [
    "`_format_list_content()` just makes sure that the values from the list are properly separated and there are no unnecessary separators at the start and end. It treats all elements but the last the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd024f1-c59e-4539-9278-00bd85a8ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_list_content(list_in):\n",
    "    out = ''\n",
    "    for i, l in enumerate(list_in):\n",
    "        out = out + l\n",
    "        if i < len(list_in) - 1:\n",
    "            out = out + ';'\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e303a-a640-4a61-a04f-db4cc14a2fb0",
   "metadata": {},
   "source": [
    "### `_set_file()`\n",
    "\n",
    "`_set_file()` asks for user input to obtain a file path. Its parameters are a text snippet to output to let the user know what their entering a value for (`text`) and whether the file must already exist (`mandatory`). \n",
    "\n",
    "`mandatory` is `False` by default, but if it is set to `True` the function then checks that the file exists and if not calls itself again to obtain a file path to an existing file. This can technically become an endless loop, but I just couldn't be bothered to do something about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a621a-24f8-4ce8-b129-b3d4da164d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_file(text, mandatory=False):\n",
    "    file_path = input(f\"path to {text}: \")\n",
    "    if mandatory:\n",
    "        if os.path.isfile(file_path):\n",
    "            return file_path\n",
    "        else:\n",
    "            _set_file(f'Provided file does not exist. Please provide a valid path to the {text}: ', True)\n",
    "    else:\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64be56f-ada5-413e-862d-a1413b07b47c",
   "metadata": {},
   "source": [
    "### `StopExecution`\n",
    "\n",
    "This only exists because I don't want Jupyter to display the entire stack trace when stopping the script, it's ugly and not neccessary in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52354321-c8dc-4530-8281-556955e67e81",
   "metadata": {},
   "source": [
    "### Things to improve\n",
    "\n",
    "Currently the output in is the order that it's added to the `tags` dictionary. Maybe sort it alphabetically instead? Or by count? Or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234a839-e5e8-4316-bfdb-93ee99b1adc0",
   "metadata": {},
   "source": [
    "### Got a GUI?\n",
    "\n",
    "Yup, try `analyse_json_gui.py` (not in the notebook though, wont' work). Dependency is `easygui`. Why? Because I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfed424-afad-4e73-b591-37a732f55236",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install easygui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faae7ac-4b8c-4827-9dbe-88635cc8ae2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d50e0c-97b1-4c14-a96d-c612563ab245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abd5bc-3a57-4d02-82e1-86cb087e48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as j:\n",
    "        data = json.load(j)  # load json data\n",
    "\n",
    "    # open a file for writing\n",
    "    with open(csv_file, 'w', encoding='utf-8') as c:\n",
    "        writer = csv.writer(c)\n",
    "\n",
    "        # write the headers\n",
    "        writer.writerow(data[0].keys())\n",
    "\n",
    "        # write the data\n",
    "        for item in data:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "\n",
    "# usage\n",
    "json_to_csv('G:\\\\My Drive\\\\python\\\\util_scripts\\\\resources\\\\wos\\\\json_20240321.json', 'output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
